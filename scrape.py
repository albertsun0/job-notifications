import os
import requests
import json
from utils import set_multiline_output, addHeader

result = ""
sources = []

with open("sources.txt", "r") as f:
    for line in f:
        sources.append(line.strip())

for source in sources:
    print("Scraping README.md from", source)
    try:
        owner = source.split("/")[-2]
        repo = source.split("/")[-1]

        cacheURL = f"cache/{owner}-{repo}.md"
        readmeEndpoint = f"https://api.github.com/repos/{owner}/{repo}/readme"

        response = requests.get(readmeEndpoint)
        url = json.loads(response.text)["download_url"]

        readme = requests.get(url).text

        if os.path.isfile(cacheURL):
            with open(cacheURL, "r") as f:
                oldReadme = f.read()

            print("loaded from cache")

            diff = []

            readme_split = readme.split("\n")
            oldReadme_split = oldReadme.split("\n")
            index = 0
            for i in range(len(readme_split)):
                if readme_split[i] != oldReadme_split[index]:
                    diff.append(readme_split[i])
                    continue
                index += 1

            if diff:
                result += "## " + source + "\n\n"
                rows = diff[0].count("|") - 1
                result += addHeader(rows) + "\n".join(diff)
                result += "\n\n"

        with open(cacheURL, "w") as f:
            f.write(readme)

    except Exception as e:
        print("ERROR SCRAPING", source, e)

if result == "":
    result = "No new jobs for now."

print(result)
set_multiline_output("SCRAPER_OUTPUTS", result)
